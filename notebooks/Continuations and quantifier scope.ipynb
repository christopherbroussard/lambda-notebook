{
 "metadata": {
  "name": "",
  "signature": "sha256:b2b8d7eb7dcf3612857555621ed905f932589985196378d88474f372958a8a0e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import imp\n",
      "imp.reload(lamb.parsing)\n",
      "reload_lamb()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Continuations and scope\n",
      "## Notebook author: Kyle Rawlins\n",
      "\n",
      "Based on Chris Barker, \"Continuations and the Nature of Quantification\", Natural Language Semantics, 2002.\n",
      "\n",
      "This notebook implements a version of Barker's account of scope using continuations.  Barker develops a semantic account of quantifier scope that does not rely at all on LF manipulations etc., but rather complicates the types of lexical items and composition rules in a systematic way to make them more general.  This notebook develops the analysis in roughly the sequence found in Barker's paper; first I show how to continuize ordinary meanings, and how to write a quantifier in this framework.  I then turn to scope ambiguities, scope freezing, and finally inverse linking / QDP-within-QDP cases.\n",
      "\n",
      "Initial note: because I do not use rule-to-rule translation, and instead use general composition rules, the predictions of the system in terms of possible composition paths are somewhat closer to those developed in Yusuke Kubota and Wataru Uegaki, \"Continuation-based semantics for Conventional Implicatures: The case of Japanese benefactives\", Proceedings of SALT 19, 2009.  Basically, a system with general rules tends to overgenerate possible composition paths that converge on the same meaning; as far as I am aware the resultant S-level meanings (filtered to be the appropriate type) are the same as Barker's system.  I will note several places where this is relevant."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Continuations are notoriously hard to explain, and I will do so here mainly by example.  I recommend using the notebook to inspect objects that are mysterious, check on the details of derivations, etc.\n",
      "\n",
      "An ordinary meaning can be continuized by turning it into a function from its continuation to itself applied to its continuation.  The continuation represents something like the \"next state\" of the computation.  The continuation of X is always a function from X to type t; the output type is because all derivations (by assumption) end in ordinary type t.\n",
      "\n",
      "The effect is perhaps easiest to see when continuizing type t, exemplified by `raining1` (the proposition that it is raining) and `raining2` (its continuized form) below.  Crucially, if the identity function is combined via function application with a continuized denotation of this type, we always get back its ordinary denotation.  This is always true of the sentence-level meanings, which I assume to be of type `<<t,t>,t>` following Barker.  The identity function can be thought of as an empty context of sorts.\n",
      "\n",
      "Two more examples for type e, and type `<e,t>`, are given as well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%lamb\n",
      "||raining1|| = Raining_t\n",
      "||raining2|| = L f_<t,t> : f(Raining_t)\n",
      "||john1|| = John_e\n",
      "||john2|| = L f_<e,t> : f(John_e)\n",
      "||cat1|| = L x_e : Cat(x)\n",
      "||cat2|| = L f_<<e,t>,t> : f(L x_e : Cat(x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Mapping any ordinary denotation to its continuation is relatively straightforward at a mechanical level, at least.  `econt` below illustrates a combinator that does this for type e."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%lamb\n",
      "econt = L x_e : L f_<e,t> : f(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "econt(john1.content).reduce_all() # we get back the content of john2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following function generalizes this; given any type it constructs the continuization combinator for that type.  This is illustrated using `cat1`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def continuize_combinator(typ):\n",
      "    cont_type = types.FunType(typ, types.type_t)\n",
      "    comb = lang.te(\"L x_%s : L f_%s : f(x)\" % (repr(typ), repr(cont_type)))\n",
      "    return comb\n",
      "\n",
      "continuize_combinator(cat1.type)(cat1.content).reduce_all()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This function could be used for e.g. constructing a typeshift or unary composition operation (as in the Kubota and Uegaki system).  Here I will use it slightly differently, to construct a 'lexical transform' that can be applied in metalanguage definitions.  This is indicated by the =<> notation below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def continuize_lex(te):\n",
      "    new_te = continuize_combinator(te.type)(te).reduce_all()\n",
      "    return new_te\n",
      "\n",
      "#continuize_lex(cat1.content)\n",
      "lamb.parsing.eq_transforms[\"cont\"] = continuize_lex"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%lamb\n",
      "||cat2|| =<cont> L x_e : Cat(x)\n",
      "||dance2|| =<cont> L x_e : Dance(x)\n",
      "||john2|| =<cont> John_e\n",
      "||the|| =<cont> L f_<e,t> : Iota x_e : f(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While in some cases standard FA can actually work and even produce effectively the right result, this isn't really what we want to do.  Note for example that given the above types there would be no way to compose `the` with `cat`.\n",
      "\n",
      "A continuized apply is somewhat complicated and easiest to see through working examples.  It needs to, effectively, sequence continuations.  The following example is a combinator that continuizes just the composition of a property (ordinary type `<e,t>`) with its argument. `b` is roughly the decontinuized version of the function, and `c` the decontinuized version of the argument; `abar` is the continuation argument for the whole expression.  Below this I illustrate this with `dance` and `john`, with some of the internal steps of the derivation revealed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%lamb\n",
      "cfaet = L f_<<<e,t>,t>,t> : L arg_<<e,t>,t> : L abar_<t,t> : f(L b_<e,t> : arg(L c_e : abar(b(c))))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(cfaet(dance2.content)(john2.content)).reduce_all().derivation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To build a compositional rule along these lines we can generate a combinator like this at arbitrary values of function type and argument type.  The following function does this by simple string substitution.\n",
      "\n",
      "*Note*: Barker implements apply as a set of rule-to-rule (i.e. category specific) operations.  I'm not personally a fan of this style (and in any case the lambda notebook doesn't currently have the infrastructure) so I will implement things via general composition rules.  This is important to keep in mind, since the implementation here overgenerates quite a bit in consequence.  This is very similar to the Kubota and Uegaki system mentioned in the intro."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def continuize_app_combinator(ftype, argtype):\n",
      "    try:\n",
      "        b = ftype.left.left\n",
      "    except: # note that exceptions other than TypeMismatch will halt composition; a TypeMismatch just signals that a particular attempt won't succeed.\n",
      "        raise types.TypeMismatch(ftype, None, \"Not a continuized function type\")\n",
      "    try:\n",
      "        abar = types.FunType(b.right, types.type_t)\n",
      "    except:\n",
      "        raise types.TypeMismatch(ftype, None, \"Not a continuized function type\")\n",
      "    try:\n",
      "        c = argtype.left.left\n",
      "    except:\n",
      "        raise types.TypeMismatch(argtype, None, \"Not a continuation type\")\n",
      "    comb_s = (\"L f_%s : L arg_%s : L abar_%s : f(L b_%s : arg(L c_%s : abar(b(c))))\" % (repr(ftype), repr(argtype), repr(abar), repr(b), repr(c)))\n",
      "    comb = te(comb_s) # parse the combinator string\n",
      "    return comb\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are a few examples of generated combinators in action."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "continuize_app_combinator(tp(\"<<<e,t>,t>,t>\"), tp(\"<<e,t>,t>\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "continuize_app_combinator(tp(\"<<<e,t>,t>,t>\"), tp(\"<<e,t>,t>\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "continuize_app_combinator(the.type, cat2.type)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Add this operation as a composition operation named `CA`.  Below this are a few examples of the rule in action."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def continuized_apply(a1, a2, assignment=None):\n",
      "    comb = continuize_app_combinator(a1.type, a2.type)\n",
      "    result = comb(a1.content.under_assignment(assignment))(a2.content.under_assignment(assignment))\n",
      "    return lang.BinaryComposite(a1, a2, result)\n",
      "\n",
      "ca_op = lang.BinaryCompositionOp(\"CA\", continuized_apply, reduce=True)\n",
      "system = lang.td_system.copy()\n",
      "system.remove_rule(\"FA\")\n",
      "system.remove_rule(\"PA\")\n",
      "system.remove_rule(\"PM\")\n",
      "system.add_rule(ca_op)\n",
      "lang.set_system(system)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(john2 * dance2).latex_step_tree()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%lamb\n",
      "||saw|| =<cont> L y_e : L x_e : Saw(x,y)\n",
      "||mary|| =<cont> Mary_e"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "john2 * (saw * mary)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(john2 * (saw * mary)).latex_step_tree()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Quantification\n",
      "\n",
      "At this point it is time to turn to quantifiers.  Items like `everyone` have continuized types, but are not generated by continuizing an ordinary meaning.  Rather, they are written as continuized types that manipulate their continuations.  In fact, their standard GQ entry is their continuized entry.  For comparison, a continuized version of ordinary \"everyone\" is given as `everyone0`.  (All of these ignore animacy.)  While these two entries for `everyone` get the same result in subject position (shown below), they do it in different ways."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%lamb\n",
      "||someone|| = L xbar_<e,t> : Exists x_e : xbar(x)\n",
      "||everyone|| = L xbar_<e,t> : Forall x_e : xbar(x)\n",
      "||everyone0|| =<cont> L f_<e,t> :  Forall x_e : f(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "everyone * (saw * mary)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "everyone0 * (saw * mary)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`everyone0` will not work in object position (as in standard approaches cf. Heim and Kratzer), but the Barker's versions will, effectively for \"free\" (once the infrastructure is accepted).  The first example shows what happens with the continuized ordinary generalized quantifer; the resulting errors are generated inside the two possible continuized apply combinators.  The other examples demonstrate Barker's quantifiers in object position."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(saw * everyone0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mary * (saw * everyone)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "everyone * (saw * someone)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Multiple quantifiers work as well, shown above.  This generates inverse scope; we will attend to surface scope shortly.  I started with inverse scope because Barker does, but I'm not aware of any real significance to this choice.\n",
      "\n",
      "To get surface scope, we need a second version of application which prioritizes continuations differently."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# assumes already continuized types\n",
      "def continuize_app_combinator2(ftype, argtype):\n",
      "    try:\n",
      "        b = ftype.left.left\n",
      "    except:\n",
      "        raise types.TypeMismatch(ftype, None, \"Not a continuation type\")\n",
      "    try:\n",
      "        abar = types.FunType(b.right, types.type_t)\n",
      "    except:\n",
      "        raise types.TypeMismatch(ftype, None, \"Not a continuized function type\")\n",
      "    try:\n",
      "        c = argtype.left.left\n",
      "    except:\n",
      "        raise types.TypeMismatch(argtype, None, \"Not a continuation type\")\n",
      "    comb_s = (\"L f_%s : L arg_%s : L abar_%s : arg(L c_%s : f(L b_%s : abar(b(c))))\" % (repr(ftype), repr(argtype), repr(abar), repr(c), repr(b)))\n",
      "    comb = te(comb_s)\n",
      "    return comb\n",
      "\n",
      "def continuized_apply2(a1, a2, assignment=None):\n",
      "    comb = continuize_app_combinator2(a1.type, a2.type)\n",
      "    result = comb(a1.content.under_assignment(assignment))(a2.content.under_assignment(assignment))\n",
      "    return lang.BinaryComposite(a1, a2, result)\n",
      "\n",
      "ca2_op = lang.BinaryCompositionOp(\"CA2\", continuized_apply2, reduce=True)\n",
      "system = lang.td_system.copy()\n",
      "system.remove_rule(\"FA\")\n",
      "system.remove_rule(\"PA\")\n",
      "system.remove_rule(\"PM\")\n",
      "system.add_rule(ca_op)\n",
      "system.add_rule(ca2_op)\n",
      "lang.set_system(system)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To try to get a sense for what is going on, here are two combinators for the simple `<e,t>` case.  At their heart, the do the same thing, apply `b` to `c`, the decontinuized function and argument respectively."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "continuize_app_combinator(the.type, cat2.type)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "continuize_app_combinator2(the.type, cat2.type)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And on to the interesting stuff.  In general each of these rules can apply to any continuized type, so now we do overgenerate the same result.  (Barker overgenerates less, because of his use of rule-to-rule translation.)  All the results are right, so this isn't really a problem.  When there are multiple quantifiers, we generate both readings."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "everyone * (saw * mary)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "everyone * (saw * someone)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(someone * (saw * everyone))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(someone * (saw * everyone))[1].latex_step_tree()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What about quantificational determiners?  Barker presents two treatments.  In the text, regular FA is allowed to combine a determiner with its complement (in a rule-to-rule fashion).  In the appendix, a different general treatment not requiring FA, but using choice functions, is presented.  For the moment I stick to the text version, and allow FA as a general possibility (so I will overgenerate more than Barker did, a la Kubota and Ueegaki).  Later we'll need the choice function version."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%lamb\n",
      "||every|| = L pbar_<<<e,t>,t>,t> : L xbar_<e,t> : pbar(L f_<e,t> : Forall x_e : (f(x) >> xbar(x)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "system = lang.td_system.copy()\n",
      "#system.remove_rule(\"FA\")\n",
      "system.remove_rule(\"PA\")\n",
      "system.remove_rule(\"PM\")\n",
      "system.add_rule(ca_op)\n",
      "system.add_rule(ca2_op)\n",
      "lang.set_system(system)\n",
      "\n",
      "def tfilter_fun(i):\n",
      "    return (i.type == lang.tp(\"<<t,t>,t>\"))\n",
      "\n",
      "tfilter = lang.CRFilter(\"S-filter\", tfilter_fun)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The latter part of the above box prunes down on a bit of overgeneration; any derivation that does not result in a sentence type (e.g. `<<t,t>,t>` in a continuized setting) is eliminated.\n",
      "\n",
      "Another form of overgeneration is that we can see that the CA rule can also apply in D-N combination, though the derivation resulting from this choice won't converge so it doesn't especially matter."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(every * cat2).latex_step_tree()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = (every * cat2) * (saw * someone)\n",
      "tfilter(r)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r[1].latex_step_tree()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Scope bounding\n",
      "\n",
      "In order to make tensed S nodes a scope island, Barker provides a different composition rule for S$\\rightarrow$NP VP nodes that blocks continuation passing.  In a setting with generalized composition rules, this needs to be done a bit differently.  One can define an operator that performs the same function as this rule, and this operator might be lexically instantiated by e.g. `that`.  I've named this operator `Disrupt`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%lamb\n",
      "||Disrupt|| = L s_<<t,t>,t> : L abar_<t,t> : abar(s(L p_t : p))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is actually somewhat hard to find examples that really use this in an extensional setting with no binding/traces; Barker provides conjunction.  I'm not (currently) treating conjunction in this fragment so we must search for something a little different.  One relevant case is the simplest analysis of embedding predicates like `it is false that` as negation.  Empirically, quantifiers cannot scope over this, though they can scope over ordinary negation.\n",
      "\n",
      "1. It is not true that some student left. ($\\neg >> \\forall$)\n",
      "2. Some student didn't leave.  ($\\forall >> \\neg$, or very marginally $\\neg >> \\forall$)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%lamb\n",
      "||iift|| =<cont> L p_t : ~p "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can first see that Disrupt has a non-substantive but visible effect on simple quantificational sentences.  In one case, the quantifier scopes under the continuation, in the other case, over."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfilter(Disrupt * (someone * dance2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfilter(someone * dance2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This now does something interesting when `iift` (_it is false that_) composes with or without Disrupt:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfilter(iift * (someone * dance2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r2 = tfilter(iift * (Disrupt * (someone * dance2)))\n",
      "r2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Negation in these derivations is scopally inert, and quantifiers scope over it.  In fact this reveals an interesting property of this system: whatever items are scopally potent must scope over whatever material is scopally inert, up to operators like Disrupt or other scopal elements.\n",
      "\n",
      "If you think about scope as a form of '_projective meaning_', then this means that in a framework like the present with only _at-issue_ meaning as the other choice, scopal elements project maximally.  This fact was exploited by Kubota and Uegaki in using continuations for Potts-style Conventional Implicature (CI) content, which should project maximally.  It is worth noting, however, that as far as I can see scope is _not_ CI content as such, and we would certainly not want scopal ambiguities to interact with projective meanings in the way that combining the two uses of continuations would involve.  (At the least, tensed finite clauses are not generally a scope island for CI content.)  In summary, it is currently unclear (to me) that continuations can be used as a _general_ mechanism for projective meaning displacement in a compositional semantics, because there are different empirical properties for different types of projective meaning; I don't yet see how it could account for both quantifier scope and CI projection at the same time.\n",
      "\n",
      "Moving on, one can generate a non-scopally-inert negation by using a similar trick to what was performed on the quantifiers, i.e. applying the content of the item after the continuation.  This is shown below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%lamb\n",
      "||sneg|| = L f_<<t,t>,t> : ~ f(L p_t : p)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now negation of this form must take widest form w.r.t. scopally inert elements, but will scope ambiguously w.r.t. scopal elements."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfilter(sneg * (someone * dance2)).eliminate_dups() # present a maximally cleaned up result for once"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Recursive DPs and inverse linking\n",
      "\n",
      "The LF analysis of scope has in general had a hard time with the scope of DPs within DPs.  First, in many cases the judgments aren't very clear.  But to the extent that they are clear, one must block some of the readings.  Barker's proposal is that the possible readings just fall out of the architecture of continuation-based scope.  Let's see how this works."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%lamb\n",
      "||no|| = L pbar_<<<e,t>,t>,t> : L xbar_<e,t> : pbar(L f_<e,t> : (~ (Exists x_e : (f(x) & xbar(x)))))\n",
      "||a|| = L pbar_<<<e,t>,t>,t> : L xbar_<e,t> : pbar(L f_<e,t> : (Exists x_e : (f(x) & xbar(x))))\n",
      "||fromP|| =<cont> L x_e : L f_<e,t> : L y_e : f(y) & From(y,x)\n",
      "||france|| =<cont> France_e\n",
      "||fcountry|| =<cont> L x_e : ForeignCountry(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Just as a sanity check, something like \"no cat from france danced\" should compose as is, and generate one scoping.  (This works!  Though with some overgeneration of composition paths.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfilter((no * (cat2 * (fromP * france))) * dance2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now the real test: what happens with\n",
      "\n",
      "1. No cat from a foreign country danced.\n",
      "2. No cat from a foreign country saw someone.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = tfilter((no * (cat2 * (fromP * (a * fcountry)))) * dance2)\n",
      "r"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Failure!  Only one scoping is generated.  What's going on?\n",
      "\n",
      "It turns out that the hack used in the body of Barker's paper to combine NPs with Ds via regular function application doesn't allow for inverse scoping.  This is unsurprising when you think about it as this operation's effect on scope is a bit like what happens with `Disrupt`: continuations are effectively trapped.\n",
      "\n",
      "We'll have to move to the version in the appendix.  Before doing that, you may want to inspect what composition paths are being found, by looking at `r`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r[0].latex_step_tree()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the appendix, Barker treats quantificational determiners as quantifiers over choice functions of type `<<e,t>,e>`, that is, functions that map properties into individuals.  It turns out, somewhat magically at first glance, that in consequence quantificational determiners are the right type to compose with property-denoting sisters via `CA` or `CA2` and generate a GQ type.  This requires staring at the derivation for a while but the basic idea is that the continuation of a GQ should be type `<e,t>`, and so its input needs to be type `e`, and a choice function for property types bridges this need with the continuized-property-denoting sister.\n",
      "\n",
      "Strictly speaking, one should do this for `everyone` and `someone` as well, but it is pedagogically simpler not to, so I'll leave them as is."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%lamb\n",
      "||a|| = L dbar_<<<e,t>,e>,t> : Exists f_<<e,t>,e> : dbar(f)\n",
      "||no|| = L dbar_<<<e,t>,e>,t> : ~(Exists f_<<e,t>,e> : dbar(f))\n",
      "||every|| = L dbar_<<<e,t>,e>,t> : (Forall f_<<e,t>,e> : dbar(f))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "every * cat2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(every * cat2)[0].latex_step_tree(derivations=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is worth pausing for a moment to contemplate the way this works in the above derivation.\n",
      "\n",
      "With this framework we can return to our set of inverse linking examples.  Note that some of these can be extremely slow to render and we are getting an exponential explosion because CA and CA2 can both apply at any stage; this can be filtered if necessary using the `eliminate_dups` function on `CompositionResult`s, but it is instructive to see all composition paths so it is not done by default."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfilter((no * (cat2 * (fromP * france))) * dance2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r = tfilter((no * (cat2 * (fromP * (a * fcountry)))) * dance2)\n",
      "r # this may be a bit slow to render..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r[0].latex_step_tree()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r2 = tfilter((no * (cat2 * (fromP * (a * fcountry)))) * (saw * everyone))\n",
      "#r2 # this may be a bit slow to render...."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is pretty hard to read, so let's cheat and ignore some of the derivation paths..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r2.eliminate_dups()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have:\n",
      " * 0: everyone >> no cat >> a foreign country\n",
      " * 1: no cat >> a foreign country >> everyone\n",
      " * 2: everyone >> a foreign country >> no cat\n",
      " * 3: a foreign country >> no cat >> everyone\n",
      " \n",
      "What is missing?\n",
      " * no cat >> everyone >> a foreign country\n",
      " * a foreign country >> everyone >> no cat\n",
      " \n",
      "These are exactly the cases where something would split the scope of the two quantifiers in the subject DP, and empirically, these readings are supposed to be absent in general (this observation is due to May).  This demonstrates what Barker calls the _Integrity Constraint_, which is that when scopal elements form a constituent together, they can't scope independently w.r.t. scopal elements outside that constituent.  Intuitively, their scope is compositionally determined inside the smallest constituent they are a member of, and can't be changed or interrupted after that.\n",
      "\n",
      "Arguably, the derivation of this constraint from extremely general principles is the most important feature of this analysis.\n",
      "\n",
      "This concludes this fragment; I have left out discussion of conjunction, and of the mathematical parts of Barker's paper."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}